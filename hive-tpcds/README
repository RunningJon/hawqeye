1. Create and exchange keys for hiveadmin for all hosts.

Example script:

#!/bin/bash
set -e

username=hiveadmin

rm -f key_node*
rm -f authorized_keys

for i in $(cat hive_hosts.txt); do
        echo $i
        counter=$(ssh $i "grep hiveadmin /etc/passwd | wc -l")
        if [ "$counter" -gt "0" ]; then
                ssh $i "userdel $username"
        fi
        ssh $i "useradd $username"
        ssh $i "echo $username:changeme | chpasswd"
        ssh $username@$i "ssh-keygen -f ~/.ssh/id_rsa -N ''"
        echo "scp $i:/home/$username/.ssh/id_rsa.pub key_$i"
        scp $i:/home/$username/.ssh/id_rsa.pub key_$i
done

echo "" > authorized_keys

for k in $(ls key_*); do
        echo "cat $k >> authorized_keys"
        cat $k >> authorized_keys
done

for i in $(cat hive_hosts.txt); do
        echo "scp authorized_keys $username@$i:/home/$username/.ssh/"
        scp authorized_keys $username@$i:/home/$username/.ssh/
done

rm -f key_node*
rm -f authorized_keys

2. After extracting this installer, create a dn.txt file in the hive-tpcds directory with an entry for every data node.
3. Run rollout.sh which will create tpcds-env.sh.
4. Edit tpcds-env.sh to set the variables you want.
4. On every datanode, create a "/datax/" directory with 1 through $DSDGEN_THREADS_PER_NODE which is set in tpcds-env.sh.  This is where data will reside and symbolic links also work.
5. On every datanode, create a sub-directory named "/datax/$TPCDS_DBNAME"

Example:

#!/bin/bash
set -e

DSDGEN_THREADS_PER_NODE="12"
TPCDS_DBNAME="tpcds_hive"
TPCDS_USERNAME="hiveadmin"

for i in $(cat dn.txt); do
	echo $i
	for x in $(seq 1 $DSDGEN_THREADS_PER_NODE); do
		echo "mkdir -p /data$x/$TPCDS_DBNAME"
		ssh $i "mkdir -p /data$x/$TPCDS_DBNAME"
		echo "chown $TPCDS_USERNAME /data$x/$TPCDS_DBNAME"
		ssh $i "chown $TPCDS_USERNAME /data$x/$TPCDS_DBNAME"
	done
done

6. Create a /user/hiveadmin directory in hdfs
su - hdfs
hdfs dfs -mkdir /user/hiveadmin
hdfs dfs -chown hiveadmin /user/hiveadmin

7. You may have to edit your core-site.xml file to set the default namenode if you are using a node that isn't a data node.

Example:
vi /etc/hadoop/conf/core-site.xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://node33.gphd.local:8020</value>
  </property>
</configuration>

8. Set Ambari according to Google Drive Doc
https://drive.google.com/open?id=1sATI700SjplbLdbVx1vuq1G8Xiqrv1wmouwxsiTSwS8

9. Add this to the hiveadmin .bashrc file
export HADOOP_CLIENT_OPTS="-Djline.terminal=jline.UnsupportedTerminal"

without this, beeline will fail if running in the background.  Strange bug.

10. Run rollout.sh to run the entire TPC-DS bencmark.
Example:
./rollout.sh

Example to run in the background:

./rollout.sh > tpcds.log 2>&1 < tpcds.log &
